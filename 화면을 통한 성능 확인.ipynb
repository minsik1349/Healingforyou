{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.9.0+cu111, Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 파이토치 계산 dvice 설정\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using PyTorch version: {torch.__version__}, Device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MishFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        ctx.save_for_backward(x)\n",
    "        return x * torch.tanh(F.softplus(x))   # x * tanh(ln(1 + exp(x)))\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x = ctx.saved_tensors[0]\n",
    "        sigmoid = torch.sigmoid(x)\n",
    "        tanh_sp = torch.tanh(F.softplus(x)) \n",
    "        return grad_output * (tanh_sp + x * sigmoid * (1 - tanh_sp * tanh_sp))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return MishFunction.apply(x)\n",
    "\n",
    "def to_Mish(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, Mish())\n",
    "        else:\n",
    "            to_Mish(child)\n",
    "            \n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            Mish(),\n",
    "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BasicBlock.expansion),)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.Mish = Mish()\n",
    "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * BasicBlock.expansion))\n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.Mish(x)\n",
    "        return x\n",
    "    \n",
    "class BottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.residual_function = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            Mish(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            Mish(),\n",
    "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        self.Mish = Mish()\n",
    "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels*BottleNeck.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels*BottleNeck.expansion))         \n",
    "    def forward(self, x):\n",
    "        x = self.residual_function(x) + self.shortcut(x)\n",
    "        x = self.Mish(x)\n",
    "        return x\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_block, num_classes=3, init_weights=True):\n",
    "        super().__init__()\n",
    "        self.in_channels=64\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            Mish(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
    "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
    "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
    "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        # weights inittialization\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.conv2_x(output)\n",
    "        x = self.conv3_x(output)\n",
    "        x = self.conv4_x(x)\n",
    "        x = self.conv5_x(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    # define weight initialization function\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                \n",
    "def resnet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def resnet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def resnet101():\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def resnet152():\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://deep-learning-study.tistory.com/299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(480, 640, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([224, 224, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gby1349\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "감정 결과:  sad\n",
      "해당 감정의 확률: 64.1 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 59.099999999999994 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 63.3 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 71.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 74.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 68.10000000000001 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 72.7 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 82.39999999999999 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 78.5 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 79.4 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 71.7 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 76.3 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 69.0 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 67.2 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 62.2 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 59.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 62.3 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.3 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.0 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.4 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.7 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.199999999999996 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 62.4 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.1 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 57.4 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 58.199999999999996 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 62.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 66.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 81.6 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 77.60000000000001 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 51.300000000000004 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 46.6 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 49.9 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 50.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 63.6 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 66.0 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.4 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 64.3 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 65.2 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 60.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.6 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 60.6 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 56.699999999999996 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 55.300000000000004 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 51.800000000000004 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 53.800000000000004 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 50.4 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 55.50000000000001 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 47.4 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 55.300000000000004 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 59.3 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 59.699999999999996 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.5 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 59.699999999999996 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.1 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.0 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 60.9 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 59.099999999999994 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 58.099999999999994 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 60.099999999999994 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 57.9 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 59.5 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 56.39999999999999 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 60.5 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 61.6 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 47.8 %\n",
      "감정 결과:  sad\n",
      "해당 감정의 확률: 54.300000000000004 %\n",
      "감정 결과:  Neturality\n",
      "해당 감정의 확률: 53.900000000000006 %\n",
      "감정 결과:  Neturality\n",
      "해당 감정의 확률: 57.699999999999996 %\n",
      "감정 결과:  Neturality\n",
      "해당 감정의 확률: 82.6 %\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 얼굴표정\n",
    "model = resnet50().to(device)\n",
    "model.load_state_dict(torch.load('best_model(정확도).pt')) \n",
    "model.eval()\n",
    "\n",
    "net = cv2.dnn.readNet(\"./yolov3-openimages.weights\", \"./yolov3-openimages.cfg\")\n",
    "\n",
    "classes = []\n",
    "with open(\"./openimages.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "colors = np.random.uniform(0, 255, size=(len(classes), 3))\n",
    "EMOTIONS = [\"Happy\" ,\"Netural\",\"Sad\"]\n",
    "\n",
    "def softmax(x):\n",
    "    array_x = x - np.max(x)\n",
    "    exp_x = np.exp(array_x) \n",
    "    result = exp_x / np.sum(exp_x)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# 카메라 지정\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened(): # 카메라가 켜져 있는 동안\n",
    "    _, img = cap.read() # 카메라 불러온 프레임 넣기\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # yolo detect face\n",
    "    try:\n",
    "        height, width, channels = img.shape\n",
    "        \n",
    "        blob = cv2.dnn.blobFromImage(img, 0.00392, (224, 224), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob) # 네트워크 입력 설정하기(블롭 객체 정의)\n",
    "        outs = net.forward(output_layers) # 네트워크 순방향 실행/추론(출력 레이어 이름)\n",
    "\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.5:\n",
    "                    # Object detected\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    # 좌표\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "                    \n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) # 노이즈 제거(Non maximum suppresion)\n",
    "\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]              # 감지된 개체를 둘러싼 사각형의 좌표\n",
    "                label = str(classes[class_ids[i]]) # 감지된 물체의 이름\n",
    "                color = colors[i]\n",
    "                if label == \"Human face\": # 라벨링 中 살마 얼굴 일경우\n",
    "                    faces = img[y : y+h, x : x+w]  # 해당 부분의 좌표로 프레임을 정제\n",
    "                    cv2.rectangle(img, (x,y ), (x + w, y + h), (255, 0 , 0), 3) # 사각형 그리기(이미지, 시작 좌표, 종료 좌표, BGR. 선두께)\n",
    "                    cv2.putText(img, label, (x, y + 30), font, 3, color, 3)\n",
    "                    \n",
    "                    faces = cv2.resize(faces, (224, 224))    # 크기 사이즈 변환 (224,224)\n",
    "                    faces = faces.astype(np.float)           # 형식 변환(float)\n",
    "                    faces = (faces/255)                      # 소수점 변환\n",
    "\n",
    "                    faces = torch.from_numpy(faces)          # tensor 형식 변환\n",
    "                    \n",
    "                    value = faces.view([1,3,224,224])        # tensor 차원 변환\n",
    "                    value = value.to(device).float()         # device 넣기 \n",
    "\n",
    "                    output = model(value)                    # 모델 넣기 \n",
    "                    _, preds = torch.max(output, 1)\n",
    "                    emotion_result = \"Happy\" if (preds == 0) else \"Neturality\" if (preds == 1)  else \"sad\"\n",
    "                    print(\"감정 결과: \", emotion_result)\n",
    "                    print(\"해당 감정의 확률: {} %\".format((softmax(output.tolist()[0]).round(3)*100).max()))\n",
    "                    # 분류 모형을 통해 표정 예측\n",
    "\n",
    "                    if (preds == 0): # happy일 경우 \n",
    "                        cv2.putText(img, \"happy\", (50, 550), font, 3, color, 3)\n",
    "                    elif (preds == 1): # 무표정일 경우\n",
    "                        cv2.putText(img, \"nothing\", (50, 550), font, 3, color, 3)\n",
    "                    else: \n",
    "                        cv2.putText(img, \"sad\", (50, 550), font, 3, color, 3)\n",
    "        \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        \n",
    "    cv2.imshow(\"image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
